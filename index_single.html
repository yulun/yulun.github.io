<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Yulun Zhang's Homepage</title>
<style type="text/css">
<!--
.STYLE8 {
	font-family: Georgia, "Times New Roman", Times, serif;
	font-size: 32px;
	font-style: italic;
	color: #000033;
}
body,td,th {
	font-family: Times New Roman, Times, serif;
	font-size: 18px;
}
.STYLE17 {font-family: Georgia, "Times New Roman", Times, serif}
.STYLE18 {font-size: 18px}
.STYLE34 {font-size: 16px}
.STYLE35 {color: #CCCCCC}
.STYLE75 {color: #0000FF}
.STYLE80 {color: #000000}
.STYLE85 {font-family: "Times New Roman", Times, serif; color: #000066; font-weight: bold; font-size: 22px; }
a:link {
	text-decoration: none;
	color: #0099FF;
}
a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: none;
	color: #009933;
}
a:active {
	text-decoration: none;
}
.STYLE90 {color: #FF0000; font-weight: bold; }
.STYLE98 {
	color: #333333;
	font-weight: bold;
	font-size: 14px;
}
.STYLE100 {color: #CC0033; font-size: 20px; font-style: italic; font-family: "Times New Roman", Times, serif;}
.STYLE109 {
	font-size: 18pt;
	color: #CC0033;
}
.STYLE122 {
	color: #F91F06;
	font-size: 16px;
}
.STYLE132 {color: #0066FF}
.STYLE136 {font-size: 22pt}
.STYLE137 {font-weight: bold; color: #333333;}
.STYLE141 {color: #0000CC; font-weight: bold; }
.STYLE157 {
	color: green;
	font-weight: bold;
}
.STYLE162 {font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: italic; }
.STYLE178 {color: green; font-weight: bold; font-size: 16pt; }
.STYLE180 {color: #FF0000}
.STYLE186 {font-weight: bold; color: #4f4f4f; }
.STYLE188 {
	color: #4f4f4f;
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
}
.STYLE189 {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
}
.STYLE193 {font-size: 14px}
.STYLE197 {font-family: Arial, Helvetica, sans-serif}
.STYLE198 {font-size: 17px; font-family: Arial, Helvetica, sans-serif; }
.STYLE200 {
	font-size: 14px;
	font-family: Arial, Helvetica, sans-serif;
	color: #4f4f4f;
}
.STYLE202 {
	color: #33CC00;
	font-weight: bold;
}
.STYLE203 {font-size: 14px; font-family: Arial, Helvetica, sans-serif; color: #4f4f4f; font-weight: bold; }
.STYLE212 {	color: #0033FF;
	font-family: "Courier New", Courier, monospace;
	font-weight: bold;
	font-size: 16;
}
.STYLE216 {color: #0033FF; font-weight: bold; font-size: 16; }
.STYLE217 {color: #0033FF; font-family: "Courier New", Courier, monospace; }
.STYLE220 {color: #333333; font-family: Arial, Helvetica, sans-serif; font-size: 14px;}
.STYLE223 {font-size: 14px; color: #666666; font-family: Arial, Helvetica, sans-serif;}
.STYLE225 {color: #FF0000; font-size: 16pt; }
.STYLE227 {font-size: 17px}
.STYLE228 {color: #008000}
.STYLE233 {font-family: "华文楷体"}
.STYLE235 {font-size: 14px; font-weight: bold; }
.STYLE237 {
	font-size: 16pt;
	color: #000000;
}
.STYLE238 {font-size: 24px}
.STYLE239 {font-size: 18pt}
.STYLE246 {font-style: italic}
.STYLE248 {font-weight: bold; color: #333333; font-size: 16pt; }
.STYLE249 {color: green}
.STYLE250 {font-weight: bold; color: #4f4f4f; font-size: 16pt; }
.STYLE251 {color: green; font-weight: bold; font-size: 18pt; }
.STYLE256 {font-size: 17px; font-family: Arial, Helvetica, sans-serif; font-weight: bold; }
.STYLE257 {
	color: #6666FF;
	font-weight: bold;
}
.STYLE258 {font-size: 18pt; color: #6666FF; }
.STYLE260 {font-size: 22pt; color: #6666FF; }
.STYLE262 {color: #6666FF}
.STYLE263 {color: #0099FF}
.STYLE265 {
	font-family: "宋体";
	font-size: 12px;
}
.STYLE266 {font-family: "华文楷体"; font-size: 14px; }
.STYLE267 {color: #333333; font-weight: bold; font-size: 14px; font-family: Arial, Helvetica, sans-serif; }
-->
</style>
</head>

<body>



<blockquote>
<table border="0" width=1050>
<tbody>
<tr>
<td width="27%" height="205"><div align="center" class="STYLE90" src="">
<div align="left"><a href="http://yulunzhang.com"><img src="./img/yulun512.png" alt="Yulun Zhang" width="223" height="234" hspace="0" vspace="0" border="0" class="STYLE35" /></a></div>
</div></td>
<td width="73%"><p><br />
<span class="STYLE85"> 
<a href="http://yulunzhang.com/"><font face="Arial" size="5"><b>Yulun Zhang </b></font></a></span><a href="http://yulunzhang.com/"><font face="Arial" size="5"><b><span class="STYLE85" lang="zh-cn">张宇伦</span></b></font></a><br />
<br />

</a></span><span class="STYLE200"><font face="Arial" size="3"><a href="https://vision.ee.ethz.ch/the-institute.html">Computer Vision Lab</a></font>, </span><span class="STYLE200"><font face="Arial" size="3">ETH Zürich</font></span><span class="STYLE200"><br />
</a></span></p>
<!--<p><span class="STYLE200"><font face="Arial" size="3">ETF D117</font><br />-->
</a></span><span class="STYLE200"><font face="Arial" size="3">ETF D114.1, Sternwartstrasse 7, 8092 Zürich, Switzerland</font><br />
</a></span></p>
<p><span class="STYLE200"><font face="Arial" size="3">Email: yulun100@gmail.com</font><br />

</span><br />

<!--<a href="cv/CV_YulunZhang.pdf"><img src="./img/cv-logo-2.png" height="30px" style="margin-bottom:-3px"></a>-->

<a href="https://github.com/yulunzhang"><img src="./img/github_PNG83_v2.png" height="30px" style="margin-bottom:-3px"></a>

<a href="https://scholar.google.com/citations?user=ORmLjWoAAAAJ&hl=en"><img src="./img/GoogleScholar.png" height="30px" style="margin-bottom:-3px"></a>

<a href="https://www.linkedin.com/in/yulun-zhang-1116b5b9/"><img src="./img/LinkedIn-Logo.png" height="30px" style="margin-bottom:-3px"></a>

<a href="http://dblp.org/pers/hd/z/Zhang:Yulun"><img src="./img/dblp.png" height="30px" style="margin-bottom:-3px"></a>

<!--
<a href="https://www.semanticscholar.org/author/Yulun-Zhang/2410227"><img src="./img/semantic_400x400.png" height="30px" style="margin-bottom:-3px"></a>
-->


<!--
<a href="https://twitter.com/intent/tweet?screen_name=yulun100&ref_src=twsrc%5Etfw" class="twitter-mention-button" data-show-count="false">Tweet to @yulun100</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<span ><a href="papers/CV_YulunZhang.pdf"><font face="Arial" size="3"><b>CV</b></font></a>&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=ORmLjWoAAAAJ&amp;hl=en"><font face="Arial" size="3"><b>Google Scholar</b></font></a>&nbsp;&nbsp; <font face="Arial" size="3"><b><a href="https://github.com/yulunzhang">Github</a></b></font>&nbsp;&nbsp; <a href="https://www.]edin.com/in/yulun-zhang-1116b5b9/"><font face="Arial" size="3"><b>LinkedIn</b></font></a></span></span><span>&nbsp;&nbsp; <a href="http://dblp.org/pers/hd/z/Zhang:Yulun"><font face="Arial" size="3"><b>DBLP</b></font></a></span></span>
-->
<br />
<br />
</span>
</p></td>
</tr>
</tbody>
</table>


<!--  
<table width="840" border="0" align="left" cellspacing="4" bordercolor="#999999">
<tr bordercolor="#333333">


<th width="60" scope="col"><a href="#Biography" class="STYLE212">Bio</a></th>

<th width="90" height="30" scope="col"><div align="center"><span class="STYLE212"><a href="#News">News</a></span></div></th>

<th width="130" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="#Education">Education</a></b></span></div></th>

<th width="160" scope="col"><div align="center"><span class="STYLE216" style="font-family: &quot;Courier New&quot;, Courier, monospace"><a href="#Research_Experiences">Experiences</a></span></div></th>

<th width="170" scope="col"><div align="center"><span class="STYLE217" style="font-size: 16"><b style="mso-bidi-font-weight: normal"><a href="#Publications">Publications</a></b></span></div></th>

<th width="110" scope="col"><div align="center"><a href="#Awards" class="STYLE212">Awards</a></div></th>

<th width="110" scope="col"><div align="center"><a href="#Academic_Service" class="STYLE212">Service</a></div></th>

<th width="110" scope="col"><div align="center"><a href="#Teaching" class="STYLE212">Teaching</a></div></th>

<th width="110" scope="col"><div align="center"><a href="#Links" class="STYLE212">Links</a></div></th>


</tr>
</table>
-->

<!-- -->

<!--<hr />

<p align="justify"><span class="subtitle1"><span style="font-size: 22pt"><b style="mso-bidi-font-weight: normal"><span 
style="FONT-FAMILY: 'Monotype Corsiva'; COLOR: green; mso-bidi-font-family: Arial"><a name="Bio" id="Bio"></a><span class="STYLE258">Biography</span></span></b></span><span class="STYLE257" style="font-size: 18pt"><span 
style="FONT-FAMILY: 'Bauhaus 93'; mso-bidi-font-family: Arial">
    <o:p></o:p>
  </span></span></span><br />
    <br />-->



</b>   
<b><font face="Arial" size="4"><a name="Biography" id="Biography"></a>Biography</font></b>
<table border="1" style="border-width: 0px;" width="1050">
<tbody>
<tr>
<td style="border-style: none; border-width: medium;">
<p style="margin-top: 3px; margin-bottom: 3px;"><font face="Arial" style="font-size: 12pt;">I am a postdoctoral researcher at <a href="https://vision.ee.ethz.ch/the-institute.html">Computer Vision Lab</a>, ETH Zürich, Switzerland, working with Prof. <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a>. Previously, I obtained my PhD degree at Department of Electrical &amp; Computer Engineering, Northeastern University, USA, in Aug. 2021. Before that I received my master degree in the Department of Automation, Tsinghua University, China, in Jul. 2017 and B.E degree from School of Electronic Engineering, Xidian University, China, in Jul. 2013.<br>
<br>
My research interest broadly includes machine learning and computer vision. Specifically, I focus on image/video restoration (e.g., super-resolution, denoising, deblurring), synthesis (e.g., style transfer, texture transfer), biomedical image enhancement and analysis,deep model compression, computational imaging (e.g., spectral compressive imaging), etc.
</font></p>
</td>
</tr>
</tbody>
</table>

<!--<span class="STYLE200"> I am a third-year PhD student at Department of Electrical &amp; Computer Engineering, Northeastern University, USA  and work with <a href="http://www1.ece.neu.edu/~yunfu/">Prof. Yun (Raymond) Fu</a> in the <a href="https://web.northeastern.edu/smilelab/"><strong>SMILE</strong></a> Lab. Before that I received my master degree in the Department of Automation, Tsinghua University, China, in Jul. 2017 and B.E degree from School of Electronic Engineering, Xidian University, China, in Jul. 2013. From Mar. 2014, I was working with my master advisor Prof. <a href="http://medialab.sz.tsinghua.edu.cn/people/YongbingZhang.html" target="_blank" rel="nofollow">Yongbing Zhang</a> on image restoration in both <a href="http://media.au.tsinghua.edu.cn/people.jsp" target="_blank" rel="nofollow">Broadband Network &amp; Digital Media Lab</a> and <a href="http://medialab.sz.tsinghua.edu.cn/" target="_blank" rel="nofollow">Shenzhen Key Lab of Broadband Network and Multimedia</a>, Tsinghua University. I was a visiting student (Jan. 2016 ~ Jul. 2016) in School of Electrical and Information Engineering, <a href="https://sydney.edu.au/">The University of Sydney</a>, Australia, under the supervision of <a href="http://sydney.edu.au/engineering/electrical/people/dong.xu/biography.html">Prof. Dong Xu</a>.</span></p>
<p align="justify"><span class="STYLE200">My research interest broadly includes sparse/collaborative representation, deep learning and their applications to computer vision tasks. Specifically, these applications lie in image processing (image restoration, generation, and style transfer) and visual recognition (face recognition/veriﬁcation).</span><span class="STYLE17"><span class="STYLE188"> I was the recipient of the <a href="img/VCIP2015best student paper-web.jpg">Best Student Paper Award</a> at IEEE International Conference on Visual Communication and Image Processing (VCIP) in  2015. </span></span></p>-->

<!--<hr />

<p align="justify"><span class="subtitle1"><span style="font-size: 22pt"><b style="mso-bidi-font-weight: normal"><span 
style="FONT-FAMILY: 'Monotype Corsiva'; COLOR: green; mso-bidi-font-family: Arial"><a name="news" id="news"></a><span class="STYLE258">News</span></span></b></span><span class="STYLE257" style="font-size: 18pt"><span 
style="FONT-FAMILY: 'Bauhaus 93'; mso-bidi-font-family: Arial">
    <o:p></o:p>
  </span></span></span></p>-->

<br>  
<b><font face="Arial" size="4"><a name="News" id="News"></a>News</font></b>
<div style="overflow-y: scroll; height:200px; width:1100px">
<table border="1" style="border-width: 0px;" width="1050">
<tbody>
<tr>
<td style="border-style: none; border-width: medium;">

<ul class="STYLE223">
<font face="Arial" size="3">

<li>11/2022: I am serving as an Area Chair for <strong>ICCV 2023</strong>. We have 2 papers accepted to <strong>AAAI 2023</strong>.</li>

<li>10/2022: I am serving as an Area Chair for <strong>CVPR 2023</strong>. We have 1 paper accepted to <strong>TIP</strong>.</li>

<li>09/2022: We have 2 papers accepted to <strong>NeurIPS 2022</strong>. They are about image restoration (<a href="https://github.com/zhengchen1999/CAT">CAT</a>) and  spectral compressive imaging (<a href="https://github.com/caiyuanhao1998/MST">DAUHST</a>).</li>

<li>07/2022: I am serving as Senior Program Committee (SPC) member for <strong>AAAI 2023</strong> and reviewer for <strong>ICLR 2023</strong>. We have 5 papers accepted to <strong>ECCV 2022</strong> and 1 paper accepted to <strong>TPAMI</strong>.</li>

<li>06/2022: We have released the codebase for <a href="https://github.com/caiyuanhao1998/MST">Spectral Compressive Imaging</a>. We have 1 paper accepted to <strong>TPAMI</strong> and 2 papers accepted to <strong>ACM MM 2022</strong>.</li>


<li>05/2022: We have 2 papers accepted to <strong>ICML 2022</strong>. Our paper <a href="https://github.com/yulunzhang/RCAN">RCAN</a> ranks top 10 most influential papers based on citations in ECCV 2018. See <a href="https://www.paperdigest.org/2022/05/most-influential-eccv-papers-2022-05/">Most Influential ECCV Papers</a>.</li>


<li>04/2022: We have 2 papers accepted to <strong>IJCAI 2022</strong>. We won the first place in <a href="https://codalab.lisn.upsaclay.fr/competitions/721">NTIRE Spectral Reconstruction Challenge</a> at CVPR, 2022. The <a href="https://arxiv.org/abs/2204.07908">paper</a> and <a href="https://github.com/caiyuanhao1998/MST-plus-plus">codebase</a> of our solution MST++ have been released.</li>

<li>03/2022: We have 3 papers (two about spectral compressive imaging, one about interpretable image SR) accepted to <strong>CVPR 2022</strong>. I am serving as reviewer for <strong>ECCV 2022</strong> and <strong>NeurIPS 2022</strong>. </li>

<li>01/2022: We have 1 paper accepted to <strong>TPAMI</strong> and another one accepted to <strong>ICLR 2022</strong>. I am serving as reviewer for <strong>ACM MM 2022</strong> and <strong>MICCAI 2022</strong>. </li>

<li>12/2021: I am serving as SPC for <strong>IJCAI 2022</strong> and reviewer for <strong>ICML 2022</strong>. </li>

<li>09/2021: 3 papers are accepted to <strong>NeurIPS 2021</strong>. I am serving as reviewer for <strong><a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></strong>. </li>

<li>07/2021: 3 papers are accepted to <strong>ICCV 2021</strong>. I accept the invitation to the novel Program Committee Board of IJCAI with service time from 2022 to 2024.</li>

<li>06/2021: 1 paper is accepted to <strong>TCYB</strong> and <strong>TIP</strong>. I am serving as reviewer for <a href="https://iclr.cc/"><strong>ICLR 2022</strong></a>.</li>

<li><font face="Arial" size="3">04/2021: Our real-world image denoising paper is accepted to <strong>IJCAI 2021</strong>. I am serving as reviewer  for <strong><a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a></strong> and <strong><a href="https://2021.acmmm.org/">ACM MM 2021</a></strong>.</font></li>

<li><font face="Arial" size="3">03/2021: Our papers about MR image enhancement and real-world image denoising are accepted to <strong>CVPR 2021</strong>. One paper about MR image super-resolution is accepted to <strong>TCSVT</strong>. </font></li>

<li><font face="Arial" size="3">02/2021: Our paper <a href="https://github.com/yulunzhang/RDN">RDN</a> ranks top 10 most influential papers based on citations in CVPR 2018. See <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">Most Influential CVPR Papers</a>. </font></li>

<li><font face="Arial" size="3">01/2021: Our <a href="https://arxiv.org/abs/2012.09243">Neural Pruning</a> paper (<a href="https://github.com/MingSun-Tse/Regularization-Pruning">code</a>) is accepted to <strong>ICLR 2021</strong>. I am serving as reviewer  for <strong><a href="http://iccv2021.thecvf.com/home">ICCV 2021</a></strong>.</font></li>

<li><font face="Arial" size="3">12/2020: I am serving as Senior Program Committee (SPC) member for <strong><a href="https://ijcai-21.org/">IJCAI 2021</a></strong> and PC member for <strong><a href="https://icml.cc/Conferences/2021">ICML 2021</a></strong>, <strong><a href="https://miccai2021.org/en/">MICCAI 2021</a></strong>.</font></li>

<li><font face="Arial" size="3">09/2020: We have 1 paper accepted to <strong>TNNLS</strong> and another one accepted to <strong>NeurIPS 2020</strong>. Code will be available soon.</font></li>

<li><font face="Arial" size="3">07/2020: We have 2 papers accepted to <strong>ECCV 2020</strong>. Code/data will be available soon.</font></li>

<li><font face="Arial" size="3">06/2020: We release the <a href="https://github.com/SHI-Labs/Pyramid-Attention-Networks">code</a> for our paper: <a href="https://arxiv.org/pdf/2004.13824.pdf">Pyramid Attention Networks for Image Restoration</a></font></li>

<li><font face="Arial" size="3">02/2020: We have 3 papers accepted to <strong>CVPR 2020</strong>. Congratulations to all authors!</font></li>

<li><font face="Arial" size="3">01/2020: We have 1 paper accepted to <strong>TPAMI</strong>.</font></li>

<li><font face="Arial" size="3">12/2019: We release the TensforFlow code and pre-trained models for ICCV19MST at <a href="https://github.com/yulunzhang/MST">MST</a>. The PyTorch code for MST is on the way.</font></li>

<li><font face="Arial" size="3">04/2019: We release all the train/test codes and pre-trained models for ICLR19RNAN at <a href="https://github.com/yulunzhang/RNAN">RNAN</a>.</font></li>

<li><font face="Arial" size="3">12/2018: We have 1 paper accepted to <strong>ICLR 2019</strong>.</font></li>

<li><font face="Arial" size="3">07/2018: PyTorch version for our CVPR18RDN has been implemented by Nguyễn Trần Toàn (trantoan060689@gmail.com) and merged into <a href="https://github.com/thstkdgus35/EDSR-PyTorch">EDSR-PyTorch</a>.</font></li>

<li><font face="Arial" size="3">07/2018: We have 1 paper accepted to <strong>ECCV 2018</strong>.</font></li>
<li><font face="Arial" size="3">07/2018: We have 1 paper accepted to <strong>ACM MM 2018</strong>.</font></li>
<li><font face="Arial" size="3">06/2018: I accept the invitation to serve as Program Committee member of <strong>AAAI 2019</strong>.</font></li>
<li><font face="Arial" size="3">02/2018: We have 1 paper accepted to <strong>CVPR 2018</strong> as spotlight. All the codes are available at <a href="https://github.com/yulunzhang/RDN">Github</a>.</font></li>
<li><font face="Arial" size="3">02/2018: I will intern to <strong><a href="https://research.adobe.com/">Adobe Research</a></strong> (San Jose, CA) this summer.</font></li>
<li><font face="Arial" size="3">07/2017: I recieve '<strong><a href="http://suisf.sz.edu.cn/20170713gg.htm">Shenzhen Universiade International Scholarship</a></strong>'.</font></li>
<li><font face="Arial" size="3">07/2017: I recieve '<strong>Excellent Master Thesis Award, Tsinghua University</strong>' award.</font></li>
<li><font face="Arial" size="3">07/2017: I recieve '<strong>Excellent Graduate in Department of Automation, Tsinghua University</strong>' award.</font></li>
<li><font face="Arial" size="3">06/2017: I recieve '<strong>Excellent Graduate of Beijing</strong>' award.</font></li>
<li><font face="Arial" size="3">05/2017: We have 1 IEEE  <strong>TSMC</strong> paper accepted.</font></li>
<li><font face="Arial" size="3">05/2017: <a href="img/CVPR17NTIRE-SecondAward.jpg">Our team (HelloSR: Xintao Wang, Yapeng Tian, Ke Yu, Yulun Zhang, Shixiang Wu, Chao Dong, Liang Lin, Yu Qiao) ranks <strong>2nd</strong> place</a> in <a href="http://www.vision.ee.ethz.ch/ntire17/">NTIRE Image Super-Resolution Challenge</a>.</font></li>

<!--<div align="left"><font face="Arial" size="3"><a href="http://yulunzhang.com/news.html" target="_blank" rel="nofollow"><em>More News</em></a></font></div>-->
</font>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<br>





<b><font face="Arial" size="4"><a name="Education" id="Education"></a>Education</font></b>
  
<ul>

<font face="Arial" size="3">


<li>Ph.D. Degree in Computer Engineering<br />
Department of ECE, <a href="http://www.northeastern.edu/">Northeastern University</a>, Boston, USA, Sep. 2017 ~ Aug. 2021<br />
Committee: Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a>, Prof. <a href="https://coe.northeastern.edu/people/camps-octavia/">Octavia Camps</a>, Prof. <a href="https://vcg.seas.harvard.edu/people/hanspeter-pfister">Hanspeter Pﬁster</a></li>

<li>M.E. Degree in Control Engineering<br />
Department of Automation, <a href="http://www.tsinghua.edu.cn/publish/newthuen/">Tsinghua University</a>, Beijing, China, Sep. 2014 ~ Jul. 2017</li>

<li>B.S. Degree in Intelligence Science and Technology<br />
School of Electronic Engineering, <a href="http://en.xidian.edu.cn/">Xidian University</a>, Xi'an, China, Sep. 2009 ~ Jul. 2013</li>
</font>
</ul>

<b><font face="Arial" size="4"><a name="Research_Experiences" id="Research_Experiences"></a>Research Experiences</font></b>

<table width="1050" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">
<tbody>

<tr>
<td width="843">

<p><font size="3" face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://vision.ee.ethz.ch/the-institute.html">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html">ETH Zürich</a>, Zürich, Switzerland</font></p>
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Postdoctoral Researcher, &nbsp; Oct. 2021 ~ Now</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a></p></font>

</td>
<td width="200" align="center">
<p><a href="https://ethz.ch/en.html"><img src="img/logoETHZ_blue_sq.png" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://web.northeastern.edu/smilelab/">SMILE Lab</a>, Northeastern University, Boston, USA</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; Sep. 2017 ~ Aug. 2021</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="https://www.northeastern.edu/"><img src="img/neu_log.png" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://vcg.seas.harvard.edu/people/hanspeter-pfister">VCG</a>, SEAS, Harvard University, Cambridge, USA</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fellow, &nbsp; May 2020 ~ Aug 2020</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://vcg.seas.harvard.edu/people">Hanspeter Pfister</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="https://www.harvard.edu/"><img src="img/Harvard_log.jpg" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://research.adobe.com/">Adobe Research</a>, USA</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Intern, &nbsp; Jun. 2019 ~ Aug. 2019</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mentors: <a href="http://web.eecs.utk.edu/~zzhang61/">Zhifei Zhang</a>, <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>, <a href="http://www.ifp.illinois.edu/~wang308/">Zhaowen Wang</a>, <a href="http://www.jiechevarria.com/">Jose Echevarria</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="https://research.adobe.com/"><img src="img/adobe_2.jpg" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://research.adobe.com/">Adobe Research</a>, USA</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Intern, &nbsp; May 2018 ~ Aug. 2018</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mentors: <a href="https://scholar.google.com/citations?user=Vu1OqIsAAAAJ&amp;hl=en">Chen Fang</a>, <a href="http://www.ifp.illinois.edu/~wang308/">Zhaowen Wang</a>, <a href="http://yilinwang.org/">Yilin Wang</a>, <a href="https://eng.ucmerced.edu/people/jyang44">Jimei Yang</a>, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a></p>

</font>
</td>
<td width="200" align="center">
<p><a href="https://research.adobe.com/"><img src="img/adobe_2.jpg" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Department of Automation, <a href="http://www.tsinghua.edu.cn/publish/newthuen/">Tsinghua University</a>, China</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; Mar. 2014 ~ Jul. 2017</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?user=0KlvTEYAAAAJ&amp;hl=en">Yongbing Zhang</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="http://www.tsinghua.edu.cn/publish/newthuen/"><img src="img/Tsinghua_University_Logo.jpg" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://english.siat.cas.cn/">Shenzhen Institutes of Advanced Technology (SIAT)</a>, <a href="https://english.cas.cn/">Chinese Academy of Science</a>, China</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; Nov. 2016 ~ Jun. 2017</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="http://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="https://english.cas.cn/"><img src="img/cas.jpeg" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; School of Electrical and Information Engineering, <a href="https://www.sydney.edu.au/">The University of Sydney</a>, Sydney, Australia</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visiting Student, &nbsp; Jan. 2016 ~ Jun. 2016</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?user=7Hdu5k4AAAAJ&amp;hl=en">Dong Xu</a> and Prof. <a href="https://wenli-vision.github.io/">Wen Li</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="https://www.sydney.edu.au/"><img src="img/220px-University_of_Sydney.jpg" height="80px"></a></p>
</td>
</tr>

<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; School of Computer Engineering, <a href="http://www.ntu.edu.sg/">Nanyang Technological University</a>, Singapore</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Project Officer, &nbsp; Nov. 2015 ~ Jan. 2016</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. Prof. <a href="https://scholar.google.com/citations?user=7Hdu5k4AAAAJ&amp;hl=en">Dong Xu</a> and Prof. <a href="https://scholar.google.com/citations?user=OhT3AWMAAAAJ&amp;hl=en">Li Niu</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="http://www.ntu.edu.sg/"><img src="img/Nanyang_Technological_University_coat_of_arms_s.jpg" height="80px"></a></p>
</td>
</tr>


<tr>
<td width="843">
<font face="Arial" size="3">
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://see.xidian.edu.cn/html/english/">School of Electronic Engineering</a>, <a href="https://en.xidian.edu.cn/">Xidian University</a>, China</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; Jun ~ Aug 2012, Dec 2012 ~ Jun 2013</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?user=TimAUN0AAAAJ&hl=en">Shuyuan Yang</a></p>
</font>
</td>
<td width="200" align="center">
<p><a href="https://en.xidian.edu.cn/"><img src="img/xidian_logo_2.png" height="80px"></a></p>
</td>
</tr>

</tbody>
</table> 



<!--
<ul>
<font face="Arial" size="3">

<li>Research Assistant<br />
  <a href="https://web.northeastern.edu/smilelab/">SMILE Lab</a>, Northeastern University, Boston, USA, Sep. 2017 ~ Now<br />
Supervisor: Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a> </li>
<br>
<li>Research Fellow<br />
  <a href="https://vcg.seas.harvard.edu/people/hanspeter-pfister">VCG</a>, SEAS, Harvard University, Cambridge, USA, May 2020 ~ Now<br />
Supervisor: Prof. <a href="https://vcg.seas.harvard.edu/people">Hanspeter Pfister</a>
</li>
<br>
<li>Research Intern<br />
  <a href="https://research.adobe.com/">Adobe Research</a>, USA, Jun. 2019 ~ Aug. 2019<br />
Mentors: <a href="http://web.eecs.utk.edu/~zzhang61/">Zhifei Zhang</a>, <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>, <a href="http://www.ifp.illinois.edu/~wang308/">Zhaowen Wang</a>, <a href="http://www.jiechevarria.com/">Jose Echevarria</a></li>
<br>
<li>Research Intern<br />
  <a href="https://research.adobe.com/">Adobe Research</a>, USA, May 2018 ~ Aug. 2018<br />
Mentors: <a href="https://scholar.google.com/citations?user=Vu1OqIsAAAAJ&amp;hl=en">Chen Fang</a>, <a href="http://www.ifp.illinois.edu/~wang308/">Zhaowen Wang</a>, <a href="http://yilinwang.org/">Yilin Wang</a>, <a href="https://eng.ucmerced.edu/people/jyang44">Jimei Yang</a>, <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a></li>
<br>
<li>Research Assistant<br />
Department of Automation, <a href="http://www.tsinghua.edu.cn/publish/newthuen/">Tsinghua University</a>, China, Mar. 2014 ~ Jul. 2017<br />
Supervisor: Prof. <a href="https://scholar.google.com/citations?user=0KlvTEYAAAAJ&amp;hl=en">Yongbing Zhang</a> </li>
<br>
<li>Research Assistant<br />
  <a href="http://english.siat.cas.cn/">Shenzhen Institutes of Advanced Technology (SIAT)</a>, Chinese Academy of Science, China, Nov. 2016 ~ Jun. 2017<br />
Supervisor: Prof. <a href="http://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a> </li>
<br>
<li>Visiting Student<br />
School of Electrical and Information Engineering, <a href="https://www.sydney.edu.au/">The University of Sydney</a>, Sydney, Australia, Jan. 2016 ~ Jun. 2016<br />
Supervisor: Prof. <a href="https://scholar.google.com/citations?user=7Hdu5k4AAAAJ&amp;hl=en">Dong Xu</a> and Prof. <a href="https://wenli-vision.github.io/">Wen Li</a></li>
<br>
<li>Project Officer <br />
School of Computer Engineering, <a href="http://www.ntu.edu.sg/">Nanyang Technological University</a>, Singapore, Nov. 2015 ~ Jan. 2016<br />
Supervisor: Prof. <a href="https://scholar.google.com/citations?user=7Hdu5k4AAAAJ&amp;hl=en">Dong Xu</a> and Prof. <a href="https://scholar.google.com/citations?user=OhT3AWMAAAAJ&amp;hl=en">Li Niu</a></li>
</font>
</ul>

<!--<hr />-->


<br>
<font face="Arial" size="4"><b><a name="Publications" id="Publications"></a>Publications</b> (<a href="https://scholar.google.com/citations?user=ORmLjWoAAAAJ&amp;hl=en">Google Scholar</a>, * : Corresponding, <sup>†</sup> : Equal contribution)</font>





<table width="1180" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">
<tbody>
<tr>
<td height="128" ><ol>

<p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Journal Papers</font></strong></span></p>
<font size="3" face="Arial">

<li><div align="justify"><p>
<strong>PFDN: Pyramid Feature Decoupling Network for Single Image Deraining</strong>  <br />
Qiang Wang, Gan Sun, Jiahua Dong, <strong>Yulun Zhang</strong> <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a> <strong>(TIP)</strong>, 2022</em>. (IF: 11.041)<br />
[<a href="https://ieeexplore.ieee.org/document/9942954">PDF</a>] [Code] 
</p></div></li>

<li><div align="justify"><p>
<strong>Lattice Network for Lightweight Image Restoration</strong>  <br />
Xiaotong Luo, Yanyun Qu, Yuan Xie, <strong>Yulun Zhang</strong>, Cuihua Li, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a> <strong>(TPAMI)</strong>, 2022</em>. (IF: 24.314)<br />
[<a href="https://ieeexplore.ieee.org/document/9847064">PDF</a>]  [Code] 
</p></div></li>

<li><div align="justify"><p>
<strong>GAN Inversion: A Survey</strong>  <br />
Weihao Xia, <strong>Yulun Zhang</strong>, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a> <strong>(TPAMI)</strong>, 2022</em>. (IF: 24.314)<br />
[<a href="https://arxiv.org/pdf/2101.05278.pdf">PDF</a>]  [<a href="https://github.com/weihaox/awesome-gan-inversion">Code</a>] </p></div></li>


<li><div align="justify"><p>
<strong>Image-Text Embedding Learning via Visual and Textual Semantic Reasoning</strong>  <br />
Kunpeng Li, <strong>Yulun Zhang*</strong>, Kai Li, Yuanyuan Li, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a> <strong>(TPAMI)</strong>, 2022</em>. (IF: 24.314)<br />
[<a href="https://ieeexplore.ieee.org/document/9706340">PDF</a>]  [<a href="https://github.com/KunpengLi1994/VSRN">Code</a>] </p></div></li>


<li><div align="justify"><p>
<strong>Accurate and Fast Image Denoising via Attention Guided Scaling</strong>  <br />
<strong>Yulun Zhang</strong>, Kunpeng Li*, Kai Li, Gan Sun, Yu Kong, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a> <strong>(TIP)</strong>, 2021</em>. (IF: 11.041)<br />
[<a href="https://ieeexplore.ieee.org/document/9479770">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Self-guided Deep Multi-view Subspace Clustering via Consensus Affinity Regularization</strong>  <br />
Kai Li, Hongfu Liu, <strong>Yulun Zhang</strong>, Kunpeng Li, and Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">IEEE Transactions on Cybernetics</a> <strong>(TCYB)</strong>, 2021</em>. (IF: 19.118)<br />
[<a href="https://ieeexplore.ieee.org/document/9478334">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Wide Weighted Attention Multi-Scale Network for Accurate MR Image Super-Resolution</strong>  <br />
Haoqian Wang, Xiaowan Hu*, Xiaole Zhao, <strong>Yulun Zhang</strong> <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE Transactions on Circuits and Systems for Video Technology</a> <strong>(TCSVT)</strong>, 2021</em>. (IF: 5.859)<br />
[<a href="https://ieeexplore.ieee.org/document/9393325">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Residual Dense Network for Image Restoration</strong>  <br />
<strong>Yulun Zhang</strong>, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a> <strong>(TPAMI)</strong>, 2020</em>. (IF: 24.314)<br />
[<a href="https://ieeexplore.ieee.org/document/8964437">PDF</a>] [<a href="https://arxiv.org/pdf/1812.10477.pdf">arXiv</a>] [<a href="https://github.com/yulunzhang/RDN/tree/master/RDN_IR/RDN_TestCode">Code</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>Vehicle and Person Re-Identification with Support Neighbor Loss</strong>  <br />
Kai Li, Zhengming Ding, Kunpeng Li, <strong>Yulun Zhang</strong>, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems </a><strong>(TNNLS)</strong>, 2020</em>. (IF: 14.255)<br />
[<a href="https://ieeexplore.ieee.org/document/9238467">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Fine-Grained Spatial Alignment Model for Person Re-Identiﬁcation with Focal Triplet Loss</strong><br />
Qinqin Zhou, Bineng Zhong, Xiangyuan Lan, Gan Sun, <strong>Yulun Zhang</strong>, Baochang Zhang, and Rongrong Ji<br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing </a><strong>(TIP)</strong>, 2020</em>. (IF: <font size="3" face="Arial">11.041</font>) <br />
[<a href="https://ieeexplore.ieee.org/document/9127793">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Continual Multi-view Task Learning via Deep Matrix Factorization</strong>  <br />
Gan Sun, Yang Cong, <strong>Yulun Zhang</strong>, Guoshuai Zhao, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems </a><strong>(TNNLS)</strong>, 2020</em>. (IF: 14.255)<br />
[<a href="https://ieeexplore.ieee.org/document/9037204">PDF</a>] [Code]
</p></div></li>

<li><div align="justify"><p>
<strong>Channel Splitting Network for Single MR Image Super-Resolution</strong>  <br />
Xiaole Zhao, <strong>Yulun Zhang</strong>, Tao Zhang, Xueming Zou <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing </a><strong>(TIP)</strong>, 2019</em>. (IF: <font size="3" face="Arial">11.041</font>)<br />
[<a href="https://arxiv.org/pdf/1810.06453.pdf">PDF</a>] [Code]
</p></div></li>

<li><div align="justify"><p>
<strong>Hierarchical Tracking by Reinforcement Learning based Searching and Coarse-to-fine Verifying</strong>  <br />
Bineng Zhong, Bing Bai, Jun Li, <strong>Yulun Zhang</strong>, Yun Fu <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing </a><strong>(TIP)</strong>,  vol. 28, no. 5, pp. 2331-2341, 2019</em>. (IF: 11.041)<br />
[<a href="https://ieeexplore.ieee.org/document/8561254">PDF</a>] [Code]
</p></div></li>

<li><div align="justify"><p>
<strong>Deep Alignment Network Based Multi-person Tracking with Occlusion and Motion Reasoning</strong>  <br />
Qinqin Zhou, Bineng Zhong, <strong>Yulun Zhang</strong>, Jun Li, Yun Fu <br />
<em><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IEEE Transactions on Multimedia </a><strong>(TMM)</strong>,  vol. xx, no. xx, pp. xxx, 2018.</em> (IF: 8.182)<br />
[<a href="https://ieeexplore.ieee.org/document/8488599">PDF</a>] [Code]
</p></div></li>

<li><div align="justify"><p>
<strong>Collaborative Representation Cascade for Single-Image Super-Resolution</strong>  <br />
Yongbing Zhang, <strong>Yulun Zhang*</strong>, Jian Zhang, Dong Xu, Yun Fu, Yisen Wang, Xiangyang Ji, Qionghai Dai <br />
<em><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221021">IEEE Transactions on Systems, Man, and Cybernetics: Systems</a><strong> (TSMC)</strong>,  vol. PP, no. 99, pp. 1-16, 2017.</em> (IF: 11.471)<br />
[<a href="/papers/TSMC-2017-CRC-early access.pdf">PDF</a>] [<a href="https://github.com/yulunzhang/CRC-SISR">Code-Github</a>] [<a href="http://pan.baidu.com/s/1dEBPvf7">Code-Baidu</a>]                
</p></div></li>

<li><div align="justify"><p>
<strong>CCR: Clustering and Collaborative Representation for Fast Single Image Super-Resolution</strong> <br />
Yongbing Zhang, <strong>Yulun Zhang*</strong>, Jian Zhang, Qionghai Dai <br />
<em><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IEEE Transactions on Multimedia </a><strong>(TMM)</strong>,  vol. 18, no. 3, pp. 405-417, Mar. 2016.</em> (IF: 8.182)<br />
[<a href="papers/TMM-2016-CCR-Zhang.pdf">PDF</a>] [<a href="https://drive.google.com/drive/folders/0B7arHHh7oU7geUlyWE5iMl9mR1E?usp=sharing">Code</a>] [<a href="https://github.com/yulunzhang/CCR-SISR">Code-Github</a>] [<a href="http://pan.baidu.com/s/1hsEw1ZQ">Code-Baidu-with mode</a>]
</p></div></li>
</font>
</ol></td>
</tr>
</tbody>
</table>







<table width="1180" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">
<tbody>
<tr>
<td><ol>

<p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Conference Papers</font></strong></span></p>
<font size="3" face="Arial">

<li><div align="justify"><p>
<strong>Spatial-Spectral Transformer for Hyperspectral Image Denoising</strong> <br />
Miaoyu Li, Ying Fu, <strong>Yulun Zhang</strong> <br />
<em>The AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), Washington DC, USA, Feb 2023.</em> <br />
[PDF] [<a href="https://github.com/MyuLi/SST">Code</a>] </p></div></li>

<li><div align="justify"><p>
<strong>Hybrid Pixel-Unshuffled Network for Lightweight Image Super-Resolution</strong> <br />
Bin Sun, <strong>Yulun Zhang</strong>, Songyao Jiang, Yun Fu <br />
<em>The AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), Washington DC, USA, Feb 2023.</em> <br />
[<a href="https://arxiv.org/abs/2203.08921v2">PDF</a>] [<a href="https://github.com/Sun1992/HPUN">Code</a>] </p></div></li>

<li><div align="justify"><p>
<strong>Cross Aggregation Transformer for Image Restoration</strong> <br />
Zheng Chen, <strong>Yulun Zhang</strong>, Jinjin Gu, Yongbing Zhang, Linghe Kong, Xin Yuan <br />
<em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), New Orleans, USA, Nov/Dec 2022.</em> (<strong>Spotlight</strong>)<br />
[<a href="https://openreview.net/pdf?id=wQ2QNNP8GtM">PDF</a>] [<a href="https://github.com/zhengchen1999/CAT">Code</a>] 
</p></div></li>

<li><div align="justify"><p>
<strong>Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging</strong> <br />
Yuanhao Cai<sup>†</sup>, Jing Lin<sup>†</sup>, Haoqian Wang, Xin Yuan, Henghui Ding, <strong>Yulun Zhang</strong>, Radu Timofte, Luc Van Gool <br />
<em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), New Orleans, USA, Nov/Dec 2022.</em> <br />
[<a href="https://arxiv.org/pdf/2205.10102.pdf">PDF</a>] [<a href="https://github.com/caiyuanhao1998/MST">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Modeling Mask Uncertainty in Hyperspectral Image Reconstruction </strong> <br />
Jiamian Wang, <strong>Yulun Zhang</strong>, Xin Yuan, Ziyi Meng, Zhiqiang Tao<br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Tel Aviv, Israel, Oct. 2022. (<strong>Oral</strong>)</em><br />
[<a href="https://arxiv.org/abs/2112.15362">PDF</a>] [<a href="https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI">Code</a>] </p></div></li>

<li><div align="justify"><p>
<strong>Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction </strong> <br />
Yuanhao Cai<sup>†</sup>, Jing Lin<sup>†</sup>, Xiaowan Hu, Haoqian Wang, Xin Yuan, <strong>Yulun Zhang</strong>, Radu Timofte, Luc Van Gool<br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Tel Aviv, Israel, Oct. 2022.</em><br />
[<a href="https://arxiv.org/pdf/2203.04845.pdf">PDF</a>] [<a href="https://github.com/caiyuanhao1998/MST">Code</a>] </p></div></li>


<li><div align="justify"><p>
<strong>Towards Interpretable Video Super-Resolution via Alternating Optimization </strong> <br />
Jiezhang Cao, Jingyun Liang, Kai Zhang, Wenguan Wang, Qin Wang, <strong>Yulun Zhang</strong>, Hao Tang, and Luc Van Gool<br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Tel Aviv, Israel, Oct. 2022.</em><br />
[<a href="https://arxiv.org/pdf/2207.10765">PDF</a>] [<a href="https://github.com/caojiezhang/DAVSR">Code</a>] </p></div></li>

<li><div align="justify"><p>
<strong>Reference-based Image Super-Resolution with Deformable Attention Transformer </strong> <br />
Jiezhang Cao, Jingyun Liang, Kai Zhang, Yawei Li, <strong>Yulun Zhang*</strong>, Wenguan Wang, and Luc Van Gool<br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Tel Aviv, Israel, Oct. 2022.</em><br />
[<a href="https://arxiv.org/pdf/2207.11938.pdf">PDF</a>] [<a href="https://github.com/caojiezhang/DATSR">Code</a>] </p></div></li>

<li><div align="justify"><p>
<strong>Super-Resolution by Predicting Offsets: An Ultra-Efficient Super-Resolution Network for Rasterized Images </strong> <br />
Jinjin Gu, Haoming Cai, Chenyu Dong, Ruofan Zhang, <strong>Yulun Zhang</strong>, Wenming Yang, Chun Yuan<br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Tel Aviv, Israel, Oct. 2022.</em><br />
[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136790572.pdf">PDF</a>] [<a href="https://github.com/HaomingCai/SRPO">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Adjustable Memory-efficient Image Super-Resolution via Individual Kernel Sparsity</strong> <br />
Xiaotong Luo, Mingliang Dai, <strong>Yulun Zhang</strong>, Yuan Xie, Ding Liu, Yanyun Qu, Yun Fu, Junping Zhang<br />
<em>ACM Multimedia (<strong>ACM MM</strong>), Lisbon, Portugal, Oct 2022.</em><br />
[<a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547768">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Cross-Modality High-Frequency Transformer for MR Image Super-Resolution</strong> <br />
Chaowei Fang, Dingwen Zhang, Liang Wang, <strong>Yulun Zhang</strong>, Lechao Cheng, Junwei Han<br />
<em>ACM Multimedia (<strong>ACM MM</strong>), Lisbon, Portugal, Oct 2022.</em><br />
[<a href="https://arxiv.org/pdf/2203.15314.pdf">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>Flow-Guided Sparse Transformer for Video Deblurring</strong> <br />
Jing Lin<sup>†</sup>, Yuanhao Cai<sup>†</sup>, Xiaowan Hu, Haoqian Wang, Youliang Yan, Xueyi Zou, Henghui Ding, <strong>Yulun Zhang</strong>, Radu Timofte, Luc Van Gool <br />
<em>International Conference on Machine Learning (<strong>ICML</strong>), Baltimore, USA, Jul 2022.</em> <br />
[<a href="https://proceedings.mlr.press/v162/lin22a/lin22a.pdf">PDF</a>] [<a href="https://github.com/linjing7/VR-Baseline">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration</strong> <br />
Jing Lin<sup>†</sup>, Xiaowan Hu<sup>†</sup>, Yuanhao Cai, Haoqian Wang, Youliang Yan, Xueyi Zou, <strong>Yulun Zhang</strong>, Luc Van Gool <br />
<em>International Conference on Machine Learning (<strong>ICML</strong>), Baltimore, USA, Jul 2022.</em> <br />
[<a href="https://proceedings.mlr.press/v162/lin22d/lin22d.pdf">PDF</a>] [<a href="https://github.com/linjing7/VR-Baseline">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>MemREIN: Rein the Domain Shift for Cross-Domain Few-Shot Learning</strong> <br />
Yi Xu, Lichen Wang, Yizhou Wang, Can Qin, <strong>Yulun Zhang</strong>, Yun Fu <br />
<em>International Joint Conferences on Artificial Intelligence (<strong>IJCAI</strong>), Vienna, Austria, Jul 2022.</em> <br />
[<a href="https://www.ijcai.org/proceedings/2022/0505.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Recent Advances on Neural Network Pruning at Initialization</strong> <br />
Huan Wang, Can Qin, Yue Bai, <strong>Yulun Zhang</strong>, Yun Fu <br />
<em>International Joint Conferences on Artificial Intelligence (<strong>IJCAI</strong>), Vienna, Austria, Jul 2022.</em> (Survey Track)<br />
[<a href="https://www.ijcai.org/proceedings/2022/0786.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction</strong> <br />
Yuanhao Cai<sup>†</sup>, Jing Lin<sup>†</sup>, Xiaowan Hu, Haoqian Wang, Xin Yuan, <strong>Yulun Zhang</strong>, Radu Timofte, Luc Van Gool <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), New Orleans, USA, Jun 2022.</em> <br />
[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.pdf">PDF</a>] [<a href="https://github.com/caiyuanhao1998/MST">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging</strong> <br />
Xiaowan Hu<sup>†</sup>, Yuanhao Cai<sup>†</sup>, Jing Lin, Haoqian Wang, Xin Yuan, <strong>Yulun Zhang</strong>, Radu Timofte, Luc Van Gool <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), New Orleans, USA, Jun 2022.</em> <br />
[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_HDNet_High-Resolution_Dual-Domain_Learning_for_Spectral_Compressive_Imaging_CVPR_2022_paper.pdf">PDF</a>] [<a href="https://github.com/caiyuanhao1998/MST">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Texture-based Error Analysis for Image Super-Resolution</strong> <br />
Salma Abdel Magid, Zudi Lin, Donglai Wei, <strong>Yulun Zhang</strong>, Jinjin Gu, Hanspeter Pfister <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), New Orleans, USA, Jun 2022.</em> <br />
[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Magid_Texture-Based_Error_Analysis_for_Image_Super-Resolution_CVPR_2022_paper.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning</strong> <br />
<strong>Yulun Zhang<sup>†</sup></strong>, Huan Wang<sup>†</sup>, Can Qin, Yun Fu <br />
<em>International Conference on Learning Representations (<strong>ICLR</strong>), Online, Apr 2022.</em> <br />
[<a href="https://openreview.net/pdf?id=AjGC97Aofee">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Aligned Structured Sparsity Learning for Efficient Image Super-Resolution</strong> <br />
<strong>Yulun Zhang<sup>†</sup></strong>, Huan Wang<sup>†</sup>, Can Qin, Yun Fu <br />
<em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), Online, Dec 2021.</em> (<strong>Spotlight</strong>)<br />
[<a href="https://proceedings.neurips.cc/paper/2021/file/15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf">PDF</a>] [<a href="https://github.com/MingSun-Tse/ASSL">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation</strong> <br />
Can Qin, Handong Zhao, Lichen Wang, Huan Wang, <strong>Yulun Zhang</strong>, Yun Fu <br />
<em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), Online, Dec 2021.</em><br />
[<a href="https://proceedings.neurips.cc/paper/2021/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf">PDF</a>] [<a href="https://github.com/canqin001/Efficient_Graph_Similarity_Computation">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training</strong> <br />
Yuanhao Cai, Xiaowan Hu, Haoqian Wang, <strong>Yulun Zhang</strong>, Hanspeter Pfister, Donglai Wei <br />
<em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), Online, Dec 2021.</em><br />
[<a href="https://papers.nips.cc/paper/2021/file/1a5b1e4daae265b790965a275b53ae50-Paper.pdf">PDF</a>] [<a href="https://github.com/caiyuanhao1998/PNGAN">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Context Reasoning Attention Network for Image Super-Resolution</strong> <br />
<strong>Yulun Zhang</strong>, Donglai Wei, Can Qin, Huan Wang*, Hanspeter Pfister, Yun Fu <br />
<em>International Conference on Learning Representations (<strong>ICCV</strong>), Online, Nov 2021.</em><br />
[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Context_Reasoning_Attention_Network_for_Image_Super-Resolution_ICCV_2021_paper.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Dynamic High-Pass Filtering and Multi-Spectral Attention for Image Super-Resolution</strong> <br />
Salma Abdel Magid, <strong>Yulun Zhang</strong>, Donglai Wei, Won-Dong Jang, Zudi Lin, Yun Fu, Hanspeter Pfister <br />
<em>International Conference on Learning Representations (<strong>ICCV</strong>), Online, Nov 2021.</em><br />
[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Magid_Dynamic_High-Pass_Filtering_and_Multi-Spectral_Attention_for_Image_Super-Resolution_ICCV_2021_paper.pdf">PDF</a>] [<a href="https://github.com/sabdelmagid/DFSA_ICCV21">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>ECACL: A Holistic Framework for Semi-Supervised Domain Adaptation</strong> <br />
Kai Li, Chang Liu, Handong Zhao, <strong>Yulun Zhang</strong>, Yun Fu <br />
<em>International Conference on Learning Representations (<strong>ICCV</strong>), Online, Nov 2021.</em><br />
[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_ECACL_A_Holistic_Framework_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf">PDF</a>] [<a href="https://github.com/kailigo/pacl">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Multi-Scale Selective Feedback Network with Dual Loss for Real Image Denoising</strong> <br />
Xiaowan Hu, Yuanhao Cai, Zhihong Liu, Haoqian Wang, <strong>Yulun Zhang</strong> <br />
<em>International Joint Conferences on Artificial Intelligence (<strong>IJCAI</strong>), Online, Aug 2021.</em> <font size="3" face="Arial"> (<strong>Oral</strong>)</font><br />
[<a href="https://www.ijcai.org/proceedings/2021/0101.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>MR Image Super-Resolution with Squeeze and Excitation Reasoning Attention Network</strong> <br />
<strong>Yulun Zhang</strong>, Kai Li, Kunpeng Li, Yun Fu <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Online, Jun 2021.</em><br />
[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_MR_Image_Super-Resolution_With_Squeeze_and_Excitation_Reasoning_Attention_Network_CVPR_2021_paper.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Pseudo 3D Auto-Correlation Network for Real Image Denoising</strong> <br />
Xiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xiaole Zhao, <strong>Yulun Zhang</strong>, Haoqian Wang <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Online, Jun 2021.</em><br />
[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Pseudo_3D_Auto-Correlation_Network_for_Real_Image_Denoising_CVPR_2021_paper.pdf">PDF</a>] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Neural Pruning via Growing Regularization</strong> <br />
Huan Wang, Can Qin, <strong>Yulun Zhang*</strong>, Yun Fu <br />
<em>International Conference on Learning Representations (<strong>ICLR</strong>), Online, May 2021.</em><br />
[<a href="https://arxiv.org/abs/2012.09243">PDF</a>] [<a href="https://github.com/MingSun-Tse/Regularization-Pruning">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Neural Sparse Representation for Image Restoration</strong> <br />
Yuchen Fan, Jiahui Yu, Yiqun Mei, <strong>Yulun Zhang</strong>, Yun Fu, Ding Liu, Thomas S Huang <br />
<em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), Dec 2020.</em><br />
[<a href="https://arxiv.org/pdf/2006.04357.pdf">PDF</a>] [<a href="https://github.com/ychfan/nsr">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Texture Hallucination for Large-Factor Painting Super-Resolution</strong> <br />
<strong>Yulun Zhang</strong>, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, Yun Fu <br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Glasgow, UK, Aug. 2020.</em><br />
[<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520205.pdf">PDF</a>] [<a href="http://yulunzhang.com/papers/PaintingSR_supp_arXiv.pdf">Supp</a>] [Code] [Data]
</p></div></li>

<li><div align="justify"><p>
<strong>LatticeNet: Towards Lightweight Image Super-resolution with Lattice Block</strong> <br />
Xiaotong Luo, Yuan Xie, <strong>Yulun Zhang</strong>, Yanyun Qu, Cuihua Li, Yun Fu <br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Glasgow, UK, Aug. 2020.</em><br />
[<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf">PDF</a>] [Supp] [Code]</p></div></li>

<li><div align="justify"><p>
<strong>Adversarial Feature Hallucination Networks for Few-Shot Learning</strong>  <br />
Kai Li, <strong>Yulun Zhang</strong>, Kunpeng Li, Yun Fu <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Seattle, USA, Jun. 2020.</em><br />
[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf">PDF</a>] [Code] </p></div></li>

<li><div align="justify"><p>
<strong>TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</strong>  <br />
Yapeng Tian, <strong>Yulun Zhang</strong>, Yun Fu, Chenliang Xu <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Seattle, USA, Jun. 2020.</em><br />
[<a href="https://arxiv.org/pdf/1812.02898.pdf">PDF</a>] [<a href="https://github.com/YapengTian/TDAN_VSR">PyTorch</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</strong>  <br />
Xiaoyu Xiang†, Yapeng Tian†, <strong>Yulun Zhang</strong>, Yun Fu, Jan Allebach, Chenliang Xu <br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Seattle, USA, Jun. 2020.</em><br />
[<a href="https://arxiv.org/pdf/2002.11616.pdf">PDF</a>] [<a href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020">PyTorch</a>]
</p></div></li>
		
<li><div align="justify"><p>
<strong>Joint Super-Resolution and Alignment of Tiny Faces</strong> <br />
Yu Yin, Joseph P. Robinson, <strong>Yulun Zhang</strong>, Yun Fu<br />
<em>The AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), New York, USA, Feb. 2020. </em><br />
[<a href="https://arxiv.org/pdf/1911.08566.pdf">PDF</a>] [<a href="https://github.com/YuYin1/JASRNet">PyTorch</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>Multimodal Style Transfer via Graph Cuts</strong> <br />
<strong>Yulun Zhang</strong>, Chen Fang, Yilin Wang, Zhaowen Wang, Zhe Lin, Yun Fu, Jimei Yang<br />
<em>International Conference on Computer Vision (<strong>ICCV</strong>), Seoul, Korea, Oct./Nov. 2019. </em><br />
[<a href="https://arxiv.org/pdf/1904.04443.pdf">PDF</a>] [<a href="papers/MST_supp_arXiv.pdf">Supp</a>] [<a href="https://github.com/yulunzhang/MST">TensorFlow</a>] [PyTorch]
</p></div></li>

<li><div align="justify"><p>
<strong>Visual Semantic Reasoning for Image-Text Matching</strong> <br />
Kunpeng Li, <strong>Yulun Zhang</strong>, Kai Li, Yuanyuan Li, Yun Fu<br />
<em>International Conference on Computer Vision (<strong>ICCV</strong>), Seoul, Korea, Oct./Nov. 2019. </em>(<strong>Oral</strong>)<br />
[<a href="https://arxiv.org/pdf/1909.02701.pdf">PDF</a>] [<a href="https://github.com/KunpengLi1994/VSRN">PyTorch</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>Attention Bridging Network for Knowledge Transfer</strong> <br />
Kunpeng Li, <strong>Yulun Zhang</strong>, Kai Li, Yuanyuan Li, Yun Fu<br />
<em>International Conference on Computer Vision (<strong>ICCV</strong>), Seoul, Korea, Oct./Nov. 2019.</em><br />
[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.pdf">PDF</a>] [Code]
</p></div></li>
		
<li><div align="justify"><p>
<strong>LRDNN: Local-reﬁning based Deep Neural Network for Person Re-Identiﬁcation with Attribute Discerning</strong> <br />
Qinqin Zhou, Bineng Zhong, Xiangyuan Lan, Gan Sun, <strong>Yulun Zhang</strong>, Mengran Gou<br />
<em>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), Macao, China, Aug. 2019.</em> <font size="3" face="Arial"> (<strong>Oral</strong>)</font><br />
[<a href="https://pdfs.semanticscholar.org/e6e0/7adffb8a021ce9beb0525540afd7c7e8612b.pdf?_ga=2.156663603.921682664.1570907140-880240021.1570907140">PDF</a>] [Code]
</p></div></li>

<li><div align="justify"><p>
<strong>Residual Non-local Attention Networks for Image Restoration </strong> <br />
<strong>Yulun Zhang</strong>, Kunpeng Li, Kai Li, Bineng Zhong, Yun Fu<br />
<em>International Conference on Learning Representations (<strong>ICLR</strong>), New Orleans, USA, May 2019.</em><br />
[<a href="https://openreview.net/pdf?id=HkeGhoA5FX">PDF</a>] [<a href="https://github.com/yulunzhang/RNAN">PyTorch</a>]
</p></div></li>
		
<li><div align="justify"><p>
<strong>Image Super-Resolution Using Very Deep Residual Channel Attention Networks </strong> <br />
<strong>Yulun Zhang</strong>, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu<br />
<em>European Conference on Computer Vision (<strong>ECCV</strong>), Munich, Germany, Sep. 2018.</em><br />
[<a href="https://arxiv.org/pdf/1807.02758.pdf">PDF</a>] [<a href="papers/ECCV-2018-RCAN_supp.pdf">Supp</a>] [<a href="https://github.com/yulunzhang/RCAN">PyTorch</a>] [Results]
</p></div></li>


        
<li><div align="justify"><p>
<strong>Support Neighbor Loss for Person Re-Identification </strong> <br />
Kai Li, Zhengming Ding, Kunpeng Li, <strong>Yulun Zhang</strong>, Yun Fu<br />
<em>ACM Multimedia (<strong>ACM MM</strong>), Seoul, Korea, Oct. 2018.</em><br />
[<a href="https://arxiv.org/abs/1808.06030">PDF</a>] [<a href="https://github.com/kailigo/SN_loss_for_reID">Code</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>Residual Dense Network for Image Super-Resolution </strong><br />
<strong>Yulun Zhang</strong>, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu<br />
<em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Salt Lake City, USA, Jun. 2018. </em>(<strong>Spotlight</strong>)<br />
[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf">PDF</a>] [<a href="https://github.com/yulunzhang/RDN">Torch</a>] [<a href="https://github.com/thstkdgus35/EDSR-PyTorch">PyTorch</a>] [<a href="https://pan.baidu.com/s/1jKmu14y">Results@BaiduDrive</a>] [<a href="https://drive.google.com/file/d/1iXv8pxPz28DmpkUQycZHeKkP1rxReIqN/view?usp=sharing">Results@GoogleDrive</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>Adaptive Local Nonparametric Regression for Fast Single Image Super-Resolution </strong> <br />
<strong>Yulun Zhang</strong>, Yongbing Zhang, Jian Zhang, Haoqian Wang, Xingzheng Wang, Qionghai Dai<br />
<em>IEEE Visual Communications and Image Processing (<strong>VCIP</strong>), Singapore, Dec. 2015.</em> (<strong>Oral</strong>) (<strong><a href="img/VCIP2015best student paper-web.jpg">Best Student Paper Award</a></strong>) <br />
[<a href="papers/VCIP-2015-ALNR-Zhang.pdf">PDF</a>] [<a href="https://drive.google.com/drive/folders/0B7arHHh7oU7geHBMcjBfd2J3N00?usp=sharing">Code</a>] [<a href="http://pan.baidu.com/s/1ckfACQ">Code-Baidu-with mode</a>]
</p></div></li>
        
<!--<li><div align="justify"><p>
<strong>Image Super-Resolution based on Dictionary Learning and Anchored Neighborhood Regression with Mutual Inconherence </strong> <br />
<strong>Yulun Zhang</strong>, Kaiyu Gu, Yongbing Zhang, Jian Zhang, Qionghai Dai <br />
<em>IEEE International Conference on Image Processing  (<strong>ICIP</strong>),  Quebec, Canada, Sep. 2015. </em>(Poster)<br />
[<a href="papers/ICIP-2015-Image Super-Resolution-Zhang.pdf">PDF</a>] 
</p></div></li>-->

<!--<li><div align="justify"><p>
<strong>Single Image Super-Resolution via Iterative Collaborative Representation </strong>  <br />
<strong>Yulun Zhang</strong>,  Yongbing Zhang, Jian Zhang, Haoqian Wang, Qionghai Dai <br />
<em>Paciﬁc-Rim Conference on Multimedia  (<strong>PCM</strong>), Gwangju, Korea, Sep. 2015. </em>(<strong>Oral</strong>)<br />   
[<a href="papers/PCM-2015-Iterative-collaborative-Zhang.pdf">PDF</a>] [<a href="https://github.com/yulunzhang/CRC-SISR">Code-Github</a>] [<a href="http://pan.baidu.com/s/1dEBPvf7">Code-Baidu</a>]            
</p></div></li>-->

<!--<li><div align="justify"><p>
<strong>Single Depth Image Super-Resolution via A Dual Sparsity Model </strong> <br />
<strong>Yulun Zhang</strong>, Yongbing Zhang,  Qionghai Dai <br />
<em>IEEE ICME Workshop on Hot Topics in 3D (<strong>Hot3D</strong>), Torino, Italy, Jun. 2015. </em>(<strong>Oral</strong>)<br />
[<a href="papers/ICME-2015-Single-depth-Zhang.pdf">PDF</a>]
</p></div></li>-->



<!--<li><div align="justify"><p>
<strong>Decompressed Video Enhancement via Accurate Regression Prior </strong> <br />
Tao Shen, <strong>Yulun Zhang</strong>, Yongbing Zhang, Xingzheng Wang, Haoqian Wang,  Qionghai Dai <br />
<em>IEEE Visual Communications and Image Processing (<strong>VCIP</strong>), Chengdu, China, Dec. 2016. </em>(<strong>Oral</strong>) <br />
[<a href="papers/VCIP-2016-Decompressed-Video-Shen.pdf">PDF</a>]
</p></div></li>-->

<!--<li><div align="justify"><p>
<strong>Single Image Super-Resolution via Projective Dictionary Learning with Anchored Neighborhood Regression</strong> <br />
Yihui Feng, Yongbing Zhang, <strong>Yulun Zhang</strong>, Tao Shen,  Qionghai Dai <br />
<em>IEEE Visual Communications and Image Processing (<strong>VCIP</strong>), Chengdu, China, Dec. 2016. </em>(<strong>Oral</strong>)<br />
[<a href="papers/VCIP-2016-Single-image-Feng.pdf">PDF</a>]
</p></div></li>-->

<!--<li><div align="justify"><p>
<strong>NTIRE 2017 Challenge on Single Image
Super-Resolution: Methods and Results </strong> [PDF]<br />
Radu Timofte, ..., <strong>Yulun Zhang</strong>, ..., et al.  <br />
<em>IEEE CVPR New Trends in Image Restoration
and Enhancement workshop and challenge on image super-resolution (<strong>CVPR NITRE</strong>), Hawaii, USA, Jul. 2017. </em> 
</p></div></li>-->
</font>
</ol></td>
</tr>
</tbody>
</table>
<!--<hr />
<p></p>-->



<table width="1180" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">
<tbody>
<tr>
<td height="128" ><ol>


<p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Workshop Papers</font></strong></span></p>
<font face="Arial" size="3">

<li><div align="justify"><p>
<strong>MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction</strong> <br />
Yuanhao Cai<sup>†</sup>, Jing Lin<sup>†</sup>, Zudi Lin, Haoqian Wang, <strong>Yulun Zhang</strong>, Hanspeter Pfister, Radu Timofte, and Luc Van Gool <br />
<em>IEEE CVPR New Trends in Image Restoration and Enhancement (NTIRE) workshop and challenge on Spectral Reconstruction from RGB (<strong>CVPR NTIRE</strong>), New Orleans, USA, Jun 2022. </em>(<strong><a href="http://yulunzhang.com/awards/NTIRE_22_Spectral_RGB_Certificate.pdf">First Place Award</a></strong>)<br />
[<a href="https://arxiv.org/abs/2204.07908">PDF</a>] [<a href="https://github.com/caiyuanhao1998/MST-plus-plus">Code</a>]</p></div></li>

<li><div align="justify"><p>
<strong>Generatively Inferential Co-Training for Unsupervised Domain Adaptation</strong> <br />
Can Qin, Lichen Wang, <strong>Yulun Zhang</strong>, Yun Fu<br />
<em>ICCV Real-World Recognition from Low-Quality Images and Videos workshop (<strong>ICCV RLQ</strong>), Seoul, Korea, Oct./Nov. 2019. </em>(<strong>Oral</strong>) (<strong><a href="http://yulunzhang.com//awards/ICCW_Award.jpeg">Best Paper Award</a></strong>)<br />
[<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/RLQ/Qin_Generatively_Inferential_Co-Training_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.pdf">PDF</a>] [<a href="https://github.com/ChinTsan01/Generatively-Inferential-Co-Training-for-UDA">Code</a>]
</p></div></li>

<li><div align="justify"><p>
<strong>NTIRE 2017 Challenge on Single Image
Super-Resolution: Methods and Results </strong> <br />
Radu Timofte, Eirikur Agustsson, Luc Van Gool, ..., Xintao Wang, Yapeng Tian, Ke Yu, <strong>Yulun Zhang</strong>, Shixiang Wu, Chao Dong, Liang Lin, Yu Qiao, ..., et al. <br />
<em>IEEE CVPR New Trends in Image Restoration
and Enhancement workshop and challenge on image super-resolution (<strong>CVPR NTIRE</strong>), Hawaii, USA, Jul. 2017. </em>(<strong><a href="http://yulunzhang.com/img/CVPR17NTIRE-SecondAward.jpg">Second Place Award</a></strong>)<br />
[<a href="papers/Timofte-CVPRW-2017.pdf">PDF</a>]</p></div></li>

</font>
</ol></td>
</tr>
</tbody>
</table>


<table width="1180" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">
<tbody>



<tr>
<td height="128" ><ol>
<p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Preprints</font></strong></span></p>
<font size="3" face="Arial">



<li><div align="justify"><p>
<strong>Pyramid Attention Networks for Image Restoration</strong> <br />
Yiqun Mei, Yuchen Fan, <strong>Yulun Zhang</strong>, Jiahui Yu, Yuqian Zhou, Ding Liu, Yun Fu, Thomas S. Huang, Humphrey Shi <br />
<em>arXiv preprint arXiv:2004.13824, 2020.</em><br />
[<a href="https://arxiv.org/pdf/2004.13824.pdf">PDF</a>] [<a href="https://github.com/SHI-Labs/Pyramid-Attention-Networks">Code</a>]</p></div></li>

<!--
<li><div align="justify"><p>
<strong>Neural Sparse Representation for Image Restoration</strong> <br />
Yuchen Fan, Jiahui Yu, Yiqun Mei, <strong>Yulun Zhang</strong>, Yun Fu, Ding Liu, Thomas S Huang <br />
<em>arXiv preprint arXiv:2006.04357, 2020.</em><br />
[<a href="https://arxiv.org/pdf/2006.04357.pdf">PDF</a>] [Code]</p></div></li>
-->


</font>
</ol></td>
</tr>

</tbody>
</table>

<br>
<b><font face="Arial" size="4"><a name="Awards" id="Awards"></a>Awards</font></b>
<ul>
<font face="Arial" size="3">

<li><a href="http://yulunzhang.com/awards/NTIRE_22_Spectral_RGB_Certificate.pdf">First Place Award</a>, Spectral Reconstruction from RGB challenge, IEEE CVPR, 2022</li>
<li><a href="http://yulunzhang.com/awards/ICCW_Award.jpeg">Best Paper Award</a>, RLQ workshop, IEEE ICCV,  2019</li>
<li>ICCV Travel Award,  2019</li>
<li>ICLR Travel Award,  2019</li>
<li>PhD Network Travel Grant, Northeastern University, USA, 2018, 2019</li>
<li>Dean's Fellowship, Northeastern University, USA, 2017</li>
<li><a href="http://suisf.sz.edu.cn/20170713gg.htm">Shenzhen Universiade International Scholarship</a>, 2017</li>
<li>Excellent Graduate of Beijing,  Beijing, 2017</li>
<li>Excellent Graduate in Department of Automation,  Tsinghua University, 2017</li>
<li>Excellent Master Thesis Award, Tsinghua University, 2017</li>
<li><a href="http://yulunzhang.com/img/CVPR17NTIRE-SecondAward.jpg">Second Place Award</a>, NTIRE workshop, IEEE CVPR,  2017</li>
<li>National Scholarship (Ministry of Education, China, Top 2%), 2016</li>
<li><a href="http://yulunzhang.com/img/VCIP2015best student paper-web.jpg">Best Student Paper Award</a> at IEEE Visual Communications and Image Processing (VCIP), 2015</li>
<li>Jingzhi Research Award in Tsinghua University, 2015</li>
</font>
</ul>



<b><font face="Arial" size="4"><a name="Academic_Service" id="Academic_Service"></a>Academic Service</font></b>

<ul>
<font face="Arial" size="3">

<span class="style8"><strong><font face="Arial" size="3" color="#808080">Area Chair</font></strong></span>
<li>Computer Vision and Pattern Recognition (CVPR), 2023</li>
<li>International Conference on Computer Vision (ICCV), 2023</li>

<br>

<span class="style8"><strong><font face="Arial" size="3" color="#808080">Senior Program Committee</font></strong></span>
<li>International Joint Conferences on Artificial Intelligence (IJCAI), 2021-2022</li>
<li>AAAI Conference on Artificial Intelligence (AAAI), 2023</li>
<!--<li>CAAI International Conference on Artificial Intelligence (CICAI), 2021</li>-->

<br>
<span class="style8"><strong><font face="Arial" size="3" color="#808080">Workshop Co-Organizer</font></strong></span>
<li>New Trends in Image Restoration and Enhancement workshop (NTIRE), CVPR 2022</li>
<!--<li>CAAI International Conference on Artificial Intelligence (CICAI), 2021</li>-->



<br>
<span class="style8"><strong><font face="Arial" size="3"><strong><font face="Arial" size="3" color="#808080">Program Committee</font></strong></font></strong></span><strong><font color="#808080"> or</font></strong><span class="style8"><strong><font face="Arial" size="3" color="#808080"> Reviewer</font></strong></span>

<li>Computer Vision and Pattern Recognition (CVPR), 2019-2022</li>
<li>International Conference on Computer Vision (ICCV), 2019-2021</li>
<li>European Conference on Computer Vision (ECCV), 2020-2022</li>
<li>International Conference on Learning Representations (ICLR), 2020-2023</li>
<li>Advances in Neural Information Processing Systems (NeurIPS), 2020-2022</li>
<li>International Conference on Machine Learning (ICML), 2021-2022</li>
<li>AAAI Conference on Artificial Intelligence (AAAI), 2019-2022</li>
<li>International Joint Conferences on Artificial Intelligence (IJCAI), 2020-2024</li>
<li>ACM International Conference on Multimedia (ACM MM), 2021-2022</li>
<li>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019-2022</li>
<!--
<li>CVPR 2019, 2020</li>
<li>ICCV 2019</li>
<li>ECCV 2020</li>
<li>NeurIPS 2020</li>
<li>AAAI 2019, 2020</li>
<li>IJCAI 2020</li>
<li>MICCAI 2020</li>
-->
<br>
<span class="style8"><strong><font face="Arial" size="3" color="#808080">Journal Reviewer</font></strong></span>
<li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</a></li>
<li><a href="https://www.springer.com/journal/11263">International Journal of Computer Vision (IJCV)</a></li>
<li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing (TIP)</a></li>
<li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</a></li>
<li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IEEE Transactions on Multimedia (TMM)</a></li>
<li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE Transactions on Circuits System and Video Technology (TCSVT)</a></li>
<li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">IEEE Transactions on Cybernetics (TCYB)</a></li>
<li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36">IEEE Transactions on Geoscience and Remote Sensing (TGRS)</a></li>
<li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852">IEEE Transactions on Computational Imaging (TCI)</a></li>
<li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42">IEEE Transactions on Medical Imaging (TMI)</a></li>
<li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=11">IEEE Transactions on Broadcasting</a></li>
<li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989">IEEE Transactions on Cognitive and Developmental Systems (TCDS)</a></li>
<li><a href="https://dl.acm.org/journal/tist">ACM Transactions on Intelligent Systems and Technology (TIST)</a></li>
<li><a href="https://www.journals.elsevier.com/pattern-recognition">Pattern Recognition (PR)</a></li>
<li><a href="https://www.sciencedirect.com/journal/information-fusion">Information Fusion (INFFUS)</a></li>
<li><a href="https://ees.elsevier.com/cviu/">Computer Vision and Image Understanding (CVIU)</a></li>
</font>
</ul>



<!--
<span class="STYLE178" style="font-family: 'Monotype Corsiva'">-</span><span class="STYLE248" style="font-family: 'Monotype Corsiva'">Attend Conferences (Oral or Poster) and Invited Talks </span> </blockquote>
<blockquote>
  <ul type="square">
    <li class="STYLE200"><strong>Invited Talk:</strong> Adaptive Local Nonparametric Regression for Fast Single Image Super‐Resolution, Singapore, Dec. 2015 </li>
    <li class="STYLE200"><strong>Invited Talk:</strong> Single Image Super-Resolution via Iterative Collaborative Representation, Gwangju, Korea, Sep. 2015 </li>
    <li class="STYLE200"><strong>Invited Talk:</strong> Single Depth Image Super-Resolution via A Dual Sparsity Model, Torino, Italy, Jun. 2015 </li>
    <li class="STYLE200">Image Super-Resolution based on Dictionary Learning and Anchored Neighborhood Regression with Mutual Inconherence, ICIP2015, Quebec, Canada, Sep. 2015 </li>
  </ul>
<span class="STYLE248" style="font-family: 'Monotype Corsiva'">-Memberships</span></blockquote>
<blockquote>
  <ul type="square">
    <li class="STYLE200">Student Member, IEEE</li>
    <li class="STYLE200">Student Member, CCF
      <span style="font-size: 22pt"><span class="STYLE257" style="font-size: 18pt"><span 
style="FONT-FAMILY: 'Bauhaus 93'; mso-bidi-font-family: Arial">
      <o:p></o:p>
      </span></span></span><br />
    </li>
  </ul>
-->
<!--<hr />-->


<!--<hr />-->

<b><font face="Arial" size="4"><a name="Teaching" id="Teaching"></a>Teaching</font></b>
<ul >
<font face="Arial" size="3">
<span class="style8"><strong><font face="Arial" size="3" color="#808080">Instructor</font></strong></span>
<li>EECE 5642: Data Visualization, Northeastern University, Spring 2020 [<a href="courses/EECE5642/syllabus-Spring2020.html">Syllabus</a>]</li>

<br>

<span class="style8"><strong><font face="Arial" size="3" color="#808080">Teaching Assistant</font></strong></span>
<li>DS 5500: Information Visualization: Applications in Data Science, Northeastern University, Spring 2021, Summer 2021</li>
<li>EECE 5639: Computer Vision, Northeastern University, Fall 2018</li>
<li>Modern Signal Processing, Tsinghua University, Fall 2015</li>
</font>
</ul>

<b><font face="Arial" size="4"><a name="Links" id="Links"></a>Links</font></b>
<ul >
<font face="Arial" size="3">

<li>Collaborators: <a href="http://media.au.tsinghua.edu.cn/qhdai.html" target="_blank" rel="nofollow">Qionghai Dai</a> (THU), <a href="https://jianzhang.tech/" target="_blank" rel="nofollow">Jian Zhang</a> (PKU), <a href="http://yapengtian.com/" target="_blank" rel="nofollow">Yapeng Tian</a> (UTD), <a href="https://sites.google.com/site/kunpengli1994/" target="_blank" rel="nofollow">KunpengLi</a> (Meta), <a href="http://kailigo.github.io/" target="_blank" rel="nofollow">Kai Li</a> (NEC), <a href="https://chintsan01.github.io/" target="_blank" rel="nofollow">Can Qin</a> (NEU), <a href="http://huanwang.tech/" target="_blank" rel="nofollow">Huag Wang</a> (NEU)
<br /></li>


<li><a href="https://openaccess.thecvf.com/menu">CVF</a></li>
<li>Journal impact: <a href="papers/journal_impact/2018JournalImpactFactor.pdf">2018</a>, <a href="papers/journal_impact/2019JournalImpactFactor.pdf">2019</a>, <a href="papers/journal_impact/2020JournalImpactFactor.pdf">2020</a>
</li>

<li>Top publications: <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computervisionpatternrecognition">Computer Vision & Pattern Recognition</a>, <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">Artificial Intelligence</a></li>
<li>Resources: <a href="https://github.com/YapengTian/Single-Image-Super-Resolution">Image Super-Resolution</a></li>
<li>Resources: <a href="https://github.com/yulunzhang/video-enhancement">Video Enhancement</a></li>

</font>

</ul>




<p class="STYLE162">Created on 2017/06/22. Last update : 2022/03/04 

<br>
<!-- Start of StatCounter Code for Default Guide -->
<!--
<script type="text/javascript">
var sc_project=11376711; 
var sc_invisible=0; 
var sc_security="f65f77d8"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript>
</noscript>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter" style="width: 0px;"
src="//c.statcounter.com/11376711/0/f65f77d8/0/" alt="web
analytics"></a></div></noscript>
-->
<!-- End of StatCounter Code for Default Guide -->
<a href="https://clustrmaps.com/site/19ncr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&cl=ffffff" style="width: 0px;" /></a>
</blockquote>

</body>
</html>
